---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.5
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# 5.1 Multiple Regression

Multiple regression involves performing linear regression with more than just one independent variable. As you may know, multiple regression with k predictors can be expressed as:

$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + \epsilon_i$$

In this equation:
- $\epsilon_i$ as the residual variance that is not explained by the model
- $\beta_0$ represents the expected value of $Y$ when $x_{i1}$ to $x_{ik}$ are 0
- $\beta_1$ signifies the change in $Y$ with a one-unit change in $x_{i1}$ while all other predictors are held constant
- The same interpretation applies to the other predictors as with $\beta_1$
 

```{admonition} Independet and dependent
:class: note
**Dependent variable**: The variable we are trying to explain with our model.  
**Independent variable(s)**: The variable(s) we use to explain the dependent variable.
```
In Python, there are various methods and packages available for conducting multiple regression. Some methods involve manual implementation, while others utilize the sklearn package or Statsmodels. For this example, we will focus on using Statsmodels.

We will work with a dataset called *trees* from the `datasets` package, which includes measurements of the girth, height, and volume of 31 felled black cherry trees.

To begin, we need to import the required packages.
```{code-cell}
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
```
Next, we will import the "trees" dataset and convert the measurements from inches, feet, and cubic feet to meters and square meters. After that, we'll take a first look at the dataset by viewing the first few rows using the "head" method.

```{code-cell}
trees = sm.datasets.get_rdataset('trees').data
df = trees
df['Girth'] = df['Girth'] * 0.0254
df['Height'] = df['Height'] * 0.3048
df['Volume'] = df['Volume'] * 0.0283168
df =df.round(3)
print(df.head())
```
Before we proceed, let's start by visualizing the data. This will help us determine the most suitable model, which in this case is linear regression. We can plot the data using either the 'seaborn' package for more advanced options, or 'matplotlib' for more basic and flexible visualizations. I opted to use the 'pairplot()' method from seaborn to create scatterplot matrices because it's just one line of code.

```{code-cell}
sns.pairplot(df);
plt.show()
```
After conducting the initial visual inspection, it is evident that there is a strong linear relationship between *Volume* and *Girth*, and a slightly weaker one between *Volume* and *Height*. However, there doesn't seem to be an obvious relationship between *Height* and *Girth*.

**Our objective is to create a linear regression model with `Volume` as the dependent variable and `Height` and `Girth` as the independent variables.**

We will use the `ols()` function from `Statsmodels` to build the multiple linear regression model. The formula is written in an R-style format, specified as `response ~ predictor(s)`. Given that we have two independent variables, they are combined with a `+` sign. Finally, the `summary()` function will provide a detailed overview of the regression results.

```{admonition} Ordinary Least Squares (OLS)
:class: note
OLS is a method used to minimize the sum of squared differences between the observed values of the dependent variable (from the dataset) and the predicted values generated by a linear function based on the independent variables.
```
```{code-cell}
model = smf.ols(formula='Volume ~ Girth + Height', data=df).fit()
print(model.summary())
```

**Understanding the Regression Summary**

The regression summary provides important information about the model, starting with the basic coefficients:

- `Intercept` ($b_0$): This represents the value of the dependent variable (Volume) when all independent variables (Girth and Height) are zero.
- `Girth`: This shows the change in Volume when Girth increases by one unit, assuming Height remains constant.
- `Height`: This indicates the change in Volume when Height increases by one unit, assuming Girth remains constant.

Each coefficient also includes:

- Standard error: This measures the accuracy of the coefficient estimate.
- t-values: These are test statistics that show how many standard errors the coefficient is away from zero.
- p-values: A small p-value suggests that the relationship between the dependent variable and the corresponding independent variable is statistically significant.

**R-squared and Adjusted R-squared**

- R-squared: This is the squared Pearson correlation, which shows the proportion of the total variance explained by the regression model. However, R-squared tends to increase as more predictors are added, even if those predictors don’t improve the model meaningfully.
- Adjusted R-squared: This is a better measure because it adjusts for the number of predictors, applying a penalty for adding unnecessary independent variables. It’s more reliable for evaluating the model’s true explanatory power.

**F-statistic and its p-value**

- F-statistic: A large F-statistic indicates that the variation explained by the model (the "explained variation") is much greater than the unexplained variation. This suggests that the independent variables are useful for predicting the dependent variable.
- The F-statistic and its p-value are calculated by comparing the fitted regression model to a model with no predictors, using analysis of variance (ANOVA). Essentially, this tells us if adding predictors improves the fit of the model.
- A significant F-statistic (often evaluated at a significance level of 0.05) means that the independent variables have real predictive power. However, in cases with many parameters, adjustments for multiple tests may require a more conservative significance level.

Now, in order to predict the tree volume, we can easily do this using the `predict()` method for the `ols()` object. To make predictions, we will provide different values for `Girth` and `Height`, which we'll store in a Pandas DataFrame called `X_predict`.
```{code-cell}
X_predict = pd.DataFrame({'Girth': [0.3, 0.4, 0.5], 
                   'Height': [20, 21, 22]})
prediction_new = model.get_prediction(X_predict)
print(prediction_new.predicted_mean)
```
The output displays the mean predicted volumes for different 'Girths' and 'Heights'. Congratulations on successfully fitting your first linear model to the dataset!

```{admonition} Summary
:class: tip

- sns.pairplot() from `seaborn` for scatterplot matrices
- ols() from `Statsmodels` for multiple linear regression

```